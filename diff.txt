diff --git a/app/graph/nodes.py b/app/graph/nodes.py
index d5e918c..aa29153 100644
--- a/app/graph/nodes.py
+++ b/app/graph/nodes.py
@@ -2,11 +2,13 @@ import logging
 import traceback
 import numpy as np
 import asyncio
+import random
 
 from typing import Dict, Any, List
 from functools import lru_cache
 from langchain_openai import ChatOpenAI, OpenAIEmbeddings
 from langchain_core.prompts import ChatPromptTemplate
+from langchain_core.messages import SystemMessage, HumanMessage
 
 from app.config import OPENAI_API_KEY, OPENAI_MODEL_ID, OPENAI_EMBEDDING_MODEL_ID
 from app.graph.utils import (
@@ -22,6 +24,8 @@ from app.utils.prompts import (
     TOPIC_TRACKER_SYSTEM_PROMPT,
     TOPIC_SHIFT_SYSTEM_PROMPT,
     DEEP_DIVE_SYSTEM_PROMPT,
+    SUMMARIZER_SYSTEM_PROMPT,
+    SILENCE_HANDLER_SYSTEM_PROMPT,
 )
 
 logger = logging.getLogger(__name__)
@@ -54,6 +58,8 @@ WEIGHT_TS_DISTANCE = 0.3
 WEIGHT_FINAL_ALGO = 0.9
 WEIGHT_FINAL_BASE = 0.1
 
+HISTORY_KEEP_LAST = 2  # è¦ç´„å¾Œã‚‚æ–‡è„ˆç¶­æŒã®ãŸã‚ã«æ®‹ã™ç›´è¿‘ã‚¿ãƒ¼ãƒ³æ•°
+
 
 @lru_cache(maxsize=1)
 def get_embeddings_model() -> OpenAIEmbeddings:
@@ -68,6 +74,40 @@ def get_embeddings_model() -> OpenAIEmbeddings:
     )
 
 
+@lru_cache(maxsize=1)
+def get_silence_chain():
+    llm = ChatOpenAI(
+        model=OPENAI_MODEL_ID,
+        temperature=0.5,
+        api_key=OPENAI_API_KEY,
+        timeout=TIMEOUT_LLM,
+    )
+    structured_llm = llm.with_structured_output(SuggestionList)
+
+    # â˜…ä¿®æ­£: speaker, listener ã‚’å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«å¤‰æ›´
+    prompt = ChatPromptTemplate.from_messages(
+        [
+            ("system", SILENCE_HANDLER_SYSTEM_PROMPT),
+            (
+                "human",
+                "ã€å…±é€šã®èˆˆå‘³ã€‘\n{common_interests}\n\nã€æ—¢å‡ºãƒˆãƒ”ãƒƒã‚¯ã€‘\n{visited}",
+            ),
+        ]
+    )
+    return prompt | structured_llm
+
+
+@lru_cache(maxsize=1)
+def get_summarizer_chain():
+    llm = ChatOpenAI(
+        model=OPENAI_MODEL_ID,
+        temperature=0.3,
+        api_key=OPENAI_API_KEY,
+        timeout=TIMEOUT_LLM,
+    )
+    return llm
+
+
 @lru_cache(maxsize=1)
 def get_topic_extractor_chain():
     """
@@ -497,16 +537,84 @@ async def silence_handler(state: ConversationState) -> Dict[str, Any]:
             - final_suggestions: æ²ˆé»™æ‰“ç ´ã®ãŸã‚ã®ææ¡ˆãƒªã‚¹ãƒˆ.
     """
     logger.info("--- Node: SilenceHandler ---")
-    # TODO: å®Ÿè£…ã¯ PR #13
-    # ãƒ€ãƒŸãƒ¼ã®ææ¡ˆã‚’è¿”ã™
-    dummy_suggestion: Suggestion = {
-        "text": "ï¼ˆæ²ˆé»™ï¼‰è©±é¡Œã‚’å¤‰ãˆã¾ã—ã‚‡ã†ã‹ï¼Ÿ",
-        "type": "silence_break",
-        "score": 1.0,
-        "speaker": "user_a",
-        "listener": "user_b",
-    }
-    return {"final_suggestions": [dummy_suggestion]}
+
+    profiles = state.get("profiles", {})
+    visited = state.get("visited_topics", [])
+    user_ids = list(profiles.keys())
+
+    if len(user_ids) < 2:
+        return {"final_suggestions": []}
+
+    # å…±é€šèˆˆå‘³ã®æŠ½å‡º (ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å…±é€šé›†åˆã‚’ã¨ã‚‹)
+    # æœ¬æ¥ã¯ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ãªã©ãŒç²¾å¯†ã ãŒã€ã“ã“ã¯é«˜é€Ÿæ€§é‡è¦–ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã•ã›ã‚‹
+    all_keywords = []
+    user_keywords = {}
+
+    for uid, prof in profiles.items():
+        kws = set()
+        for c in prof.get("interest_clusters", []):
+            kws.update(c.get("keywords", []))
+        # SNSãƒ‡ãƒ¼ã‚¿ã‚‚åŠ å‘³
+        kws.update(prof.get("sns_data", {}).get("likes", []))
+        user_keywords[uid] = kws
+        all_keywords.extend(list(kws))
+
+    # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™
+    common_interests = set()
+    uids = list(user_keywords.keys())
+    if len(uids) >= 2:
+        common_interests = user_keywords[uids[0]].intersection(user_keywords[uids[1]])
+
+    # å…±é€šãŒãªã‘ã‚Œã°å…¨å“¡ã®èˆˆå‘³ã‚’ãƒ—ãƒ¼ãƒ«ã™ã‚‹
+    target_interests = list(common_interests) if common_interests else all_keywords
+
+    interests_str = ", ".join(target_interests[:10])
+    visited_str = ", ".join(visited)
+
+    speaker = random.choice(user_ids)
+    others = [u for u in user_ids if u != speaker]
+    listener = others[0] if others else speaker
+
+    try:
+        chain = get_silence_chain()
+
+        result: SuggestionList = await chain.ainvoke(
+            {
+                "common_interests": interests_str,
+                "visited": visited_str,
+                "speaker": speaker,
+                "listener": listener,
+            },
+            config={"timeout": TIMEOUT_LLM},
+        )
+
+        final_suggestions = []
+        for item in result.suggestions:
+            final_suggestions.append(
+                {
+                    "text": item.text,
+                    "type": "silence_break",
+                    "score": item.score,
+                    "speaker": speaker,  # æ˜ç¢ºã«ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã¨ã™ã‚‹
+                    "listener": listener,
+                }
+            )
+
+        return {"final_suggestions": final_suggestions}
+
+    except Exception as e:
+        logger.error(f"Error in SilenceHandler: {e}")
+        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼è¦–ç‚¹ã€ã«å¤‰æ›´
+        fallback = [
+            {
+                "text": "ãã†ã„ãˆã°ã€æœ€è¿‘é¢ç™½ã„æ˜ ç”»ã¨ã‹è¦³ã¾ã—ãŸï¼Ÿ",
+                "type": "silence_break",
+                "score": 0.5,
+                "speaker": speaker,
+                "listener": listener,
+            }
+        ]
+        return {"final_suggestions": fallback}
 
 
 async def summarizer(state: ConversationState) -> Dict[str, Any]:
@@ -523,8 +631,44 @@ async def summarizer(state: ConversationState) -> Dict[str, Any]:
             - history_window: ç©ºãƒªã‚¹ãƒˆï¼ˆã¾ãŸã¯åœ§ç¸®å¾Œã®ãƒªã‚¹ãƒˆï¼‰.
     """
     logger.info("--- Node: Summarizer ---")
-    # TODO: å®Ÿè£…ã¯ PR #13
-    return {"summary": "summary_updated", "history_window": []}
+
+    history = state.get("history_window", [])
+    current_summary = state.get("summary", "")
+
+    # è¦ç´„å¯¾è±¡: ç›´è¿‘Nä»¶ã‚’é™¤ãå¤ã„å±¥æ­´
+    to_summarize = history[:-HISTORY_KEEP_LAST]
+    # æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ã«æ®‹ã™å±¥æ­´
+    to_keep = history[-HISTORY_KEEP_LAST:]
+
+    if not to_summarize:
+        return {}
+
+    lines = "\n".join([f"{h['speaker']}: {h['text']}" for h in to_summarize])
+
+    llm = get_summarizer_chain()
+
+    try:
+        messages = [
+            SystemMessage(content=SUMMARIZER_SYSTEM_PROMPT),
+            HumanMessage(
+                content=f"ã€ç¾åœ¨ã®è¦ç´„ã€‘\n{current_summary}\n\nã€æ–°ã—ã„ä¼šè©±å±¥æ­´ã€‘\n{lines}"
+            ),
+        ]
+
+        response = await llm.ainvoke(messages)
+        new_summary = response.content
+
+        logger.info(
+            f"Summary updated. Length: {len(current_summary)} -> {len(new_summary)}"
+        )
+
+        # Stateæ›´æ–°: summaryã‚’æ›´æ–°ã—ã€history_windowã‚’çŸ­ç¸®ã—ãŸã‚‚ã®ã«ç½®ãæ›ãˆã‚‹
+        return {"summary": new_summary, "history_window": to_keep}
+
+    except Exception as e:
+        logger.error(f"Error in Summarizer: {e}")
+        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä½•ã‚‚ã›ãšï¼ˆå±¥æ­´ã¯é•·ã„ã¾ã¾ï¼‰æ¬¡ã«é€²ã‚€
+        return {}
 
 
 async def topic_tracker(state: ConversationState) -> Dict[str, Any]:
diff --git a/app/graph/workflow.py b/app/graph/workflow.py
index c3fa45e..699f660 100644
--- a/app/graph/workflow.py
+++ b/app/graph/workflow.py
@@ -14,7 +14,7 @@ from app.graph.nodes import (
 
 logger = logging.getLogger(__name__)
 
-HISTORY_THRESHOLD = 10
+HISTORY_THRESHOLD = 8
 
 
 def route_signal(state: ConversationState) -> str:
diff --git a/app/utils/prompts.py b/app/utils/prompts.py
index 3dd88e8..e95df70 100644
--- a/app/utils/prompts.py
+++ b/app/utils/prompts.py
@@ -82,3 +82,31 @@ TOPIC_SHIFT_SYSTEM_PROMPT = """
 ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
 æŒ‡å®šã•ã‚ŒãŸJSONã‚¹ã‚­ãƒ¼ãƒï¼ˆSuggestionListï¼‰ã«å¾“ã£ã¦ãã ã•ã„ã€‚
 """
+
+SILENCE_HANDLER_SYSTEM_PROMPT = """
+ã‚ãªãŸã¯ä¼šè©±ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ãŒã€å‡ºåŠ›ã™ã‚‹ã®ã¯**ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªèº«ã®ç™ºè©±**ã§ã™ã€‚
+ä¼šè©±ãŒé€”åˆ‡ã‚Œã¦æ²ˆé»™ï¼ˆSilenceï¼‰ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚
+å…±é€šã®èˆˆå‘³ï¼ˆCommon Interestsï¼‰ã«åŸºã¥ãã€**{speaker} ãŒ {listener} ã«å‘ã‹ã£ã¦è©±ã—ã‹ã‘ã‚‹** è‡ªç„¶ãªè¨€è‘‰ã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+
+ã€ãƒ«ãƒ¼ãƒ«ã€‘
+1. **AIã‚„å¸ä¼šè€…ã®äººæ ¼ã‚’å‡ºã•ãªã„ã§ãã ã•ã„**ã€‚ã€ŒãŠäºŒäººã¯ã€œã€ã‚„ã€Œè©±é¡Œã‚’å¤‰ãˆã¾ã—ã‚‡ã†ã€ã¯ç¦æ­¢ã§ã™ã€‚
+2. **è‡ªç„¶ãªå°å…¥å¥**ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
+   - è‰¯ã„ä¾‹: ã€Œãã†ã„ãˆã°ã€{listener}ã•ã‚“ã¯ã€œå¥½ãã§ã—ãŸã‚ˆã­ï¼Ÿã€ã€Œã¨ã“ã‚ã§ã€æœ€è¿‘ã€œã«ã¯è¡Œãã¾ã—ãŸï¼Ÿã€
+   - æ‚ªã„ä¾‹: ã€Œã€œã«ã¤ã„ã¦ã¯ã©ã†ã§ã™ã‹ï¼Ÿã€ï¼ˆå”çªï¼‰
+3. Speakerè‡ªèº«ã®èˆˆå‘³ã§ã‚‚ã‚ã‚‹ãŸã‚ã€å…±æ„Ÿã‚’æ±‚ã‚ãŸã‚Šã€è»½ãè³ªå•ã™ã‚‹ãƒˆãƒ¼ãƒ³ã«ã—ã¦ãã ã•ã„ã€‚
+4. å‡ºåŠ›ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
+
+ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
+æŒ‡å®šã•ã‚ŒãŸJSONã‚¹ã‚­ãƒ¼ãƒï¼ˆSuggestionListï¼‰ã«å¾“ã£ã¦ãã ã•ã„ã€‚
+"""
+
+SUMMARIZER_SYSTEM_PROMPT = """
+ã‚ãªãŸã¯ä¼šè©±ã®è¨˜éŒ²ä¿‚ã§ã™ã€‚
+ã“ã‚Œã¾ã§ã®ã€Œä¼šè©±ã®è¦ç´„ï¼ˆCurrent Summaryï¼‰ã€ã¨ã€Œæ–°ã—ã„ä¼šè©±å±¥æ­´ï¼ˆNew Linesï¼‰ã€ã‚’çµ±åˆã—ã€ã‚ˆã‚Šç°¡æ½”ãªæ–°ã—ã„è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
+
+ã€ãƒ«ãƒ¼ãƒ«ã€‘
+1. é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã€æ±ºå®šäº‹é …ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã‚„å¥½ã¿ã®å¤‰åŒ–ã‚’æ®‹ã—ã¦ãã ã•ã„ã€‚
+2. äº›æœ«ãªæŒ¨æ‹¶ã‚„ç›¸æ§Œã¯å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚
+3. "User Aã¯ã€œã¨è¨€ã£ãŸ" ã®ã‚ˆã†ãªå®¢è¦³çš„ãªæ›¸ãæ–¹ã‚’ã—ã¦ãã ã•ã„ã€‚
+4. å‡ºåŠ›ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
+"""
diff --git a/scripts/test_optimization.py b/scripts/test_optimization.py
new file mode 100644
index 0000000..a11ea7c
--- /dev/null
+++ b/scripts/test_optimization.py
@@ -0,0 +1,209 @@
+import asyncio
+import os
+import sys
+import logging
+
+# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¸ã®ãƒ‘ã‚¹è¨­å®š
+# (å®Ÿè¡Œç’°å¢ƒã«åˆã‚ã›ã¦é©å®œèª¿æ•´ã—ã¦ãã ã•ã„)
+sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
+
+# å®Ÿéš›ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆã«åˆã‚ã›ã¦importã—ã¦ãã ã•ã„
+# ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ãƒ€ãƒŸãƒ¼ç­‰ã®èª¿æ•´ãŒå¿…è¦ã§ã™
+try:
+    from app.graph.workflow import route_signal
+    from app.graph.nodes import summarizer, silence_handler
+    from app.graph.state import get_initial_state
+except ImportError:
+    # ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€importã§ããªã„å ´åˆã®ãƒ€ãƒŸãƒ¼å®šç¾©ã‚’å…¥ã‚Œã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™
+    # ã“ã“ã§ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ§˜ã®ç’°å¢ƒãŒæ•´ã£ã¦ã„ã‚‹å‰æã§é€²ã‚ã¾ã™
+    pass
+
+# ãƒ­ã‚°è¨­å®š: å†…éƒ¨å‹•ä½œãŒè¦‹ãˆã‚‹ã‚ˆã†ã«èª¿æ•´
+logging.basicConfig(level=logging.INFO, format="%(message)s")
+logging.getLogger("app.graph.nodes").setLevel(logging.INFO)
+logging.getLogger("httpx").setLevel(logging.WARNING)
+logging.getLogger("openai").setLevel(logging.WARNING)
+
+
+async def main():
+    print("\n==================================================")
+    print("   Optimization Logic Test")
+    print("   Target: Router, Summarizer, SilenceHandler")
+    print("==================================================\n")
+
+    state = get_initial_state()
+    user_alice = "Alice"
+    user_bob = "Bob"
+
+    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆå…±é€šç‚¹ï¼šæ˜ ç”»ã€æ—…è¡Œï¼‰
+    state["profiles"] = {
+        user_alice: {
+            "user_id": user_alice,
+            "interest_clusters": [{"keywords": ["Movie", "Action", "Travel"]}],
+            "sns_data": {"likes": ["Cinema", "Kyoto"]},
+        },
+        user_bob: {
+            "user_id": user_bob,
+            "interest_clusters": [{"keywords": ["Movie", "Popcorn", "Hot Spring"]}],
+            "sns_data": {"likes": ["Netflix", "Travel"]},
+        },
+    }
+
+    # --------------------------------------------------
+    # Case 1: Summarizer Test (å±¥æ­´åœ§ç¸®)
+    # --------------------------------------------------
+    print("ğŸ§ª [Test Case 1] Summarizer (Maintenance Path)")
+    print("   Condition: History length >= Threshold (8)")
+
+    # â˜…å¤‰æ›´ç‚¹: forãƒ«ãƒ¼ãƒ—ã§ã¯ãªãã€æ–‡è„ˆã®ã‚ã‚‹å…·ä½“çš„ãªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒãƒˆã—ã¾ã™
+    # é–¾å€¤(8)ã‚’è¶…ãˆã‚‹ã‚ˆã†ã«10ä»¶ç”¨æ„
+    dummy_history = [
+        {
+            "speaker": user_alice,
+            "text": "ã­ãˆã€æœ€è¿‘ä½•ã‹é¢ç™½ã„æ˜ ç”»è¦³ãŸï¼Ÿ",
+            "timestamp": 1000,
+        },
+        {
+            "speaker": user_bob,
+            "text": "ã‚ã‚ã€å…ˆé€±å…¬é–‹ã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ˜ ç”»ã€ã™ã”ãè‰¯ã‹ã£ãŸã‚ˆï¼",
+            "timestamp": 1001,
+        },
+        {
+            "speaker": user_alice,
+            "text": "ã¸ãˆã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¥½ããªã‚“ã ã€‚ç§ã‚‚ãŸã¾ã«è¦³ã‚‹ã‚ˆã€‚",
+            "timestamp": 1002,
+        },
+        {
+            "speaker": user_bob,
+            "text": "æ˜ ç”»é¤¨ã§é£Ÿã¹ã‚‹ãƒãƒƒãƒ—ã‚³ãƒ¼ãƒ³ãŒæœ€é«˜ãªã‚“ã ã‚ˆã­ã€‚",
+            "timestamp": 1003,
+        },
+        {
+            "speaker": user_alice,
+            "text": "ã‚ã‹ã‚‹ï¼æ˜ ç”»é¤¨ã®é›°å›²æ°—ã„ã„ã‚ˆã­ã€‚ãã†ã„ãˆã°æ—…è¡Œã¯ï¼Ÿ",
+            "timestamp": 1004,
+        },
+        {
+            "speaker": user_bob,
+            "text": "æœ€è¿‘è¡Œã‘ã¦ãªã„ãªã‚ã€‚æ¸©æ³‰ã¨ã‹è¡ŒããŸã„ã€‚",
+            "timestamp": 1005,
+        },
+        {
+            "speaker": user_alice,
+            "text": "äº¬éƒ½ã®æ¸©æ³‰ã¨ã‹ã©ã†ï¼Ÿã“ã‚Œã‹ã‚‰ã®å­£ç¯€ã„ã„ã‹ã‚‚ã€‚",
+            "timestamp": 1006,
+        },
+        {
+            "speaker": user_bob,
+            "text": "ã„ã„ã­ãˆã€äº¬éƒ½ã€‚Netflixã§äº¬éƒ½ãŒèˆå°ã®æ˜ ç”»è¦³ã¦è¡ŒããŸããªã£ã¦ãŸã‚“ã ã€‚",
+            "timestamp": 1007,
+        },
+        {
+            "speaker": user_alice,
+            "text": "ã‚ã€ãã‚Œç§ã‚‚è¦³ãŸã‹ã‚‚ï¼æ™¯è‰²ç¶ºéº—ã ã£ãŸã‚ˆã­ã€‚",
+            "timestamp": 1008,
+        },
+        {
+            "speaker": user_bob,
+            "text": "ãã†ãã†ã€‚ã‚„ã£ã±ã‚Šå®Ÿéš›ã«ç¾åœ°ã«è¡ŒããŸã„ãªã‚ã€‚",
+            "timestamp": 1009,
+        },
+    ]
+
+    state["history_window"] = dummy_history
+    state["summary"] = "ä¼šè©±é–‹å§‹ã€‚"  # åˆæœŸã‚µãƒãƒªãƒ¼
+    state["input_type"] = "text"  # é€šå¸¸å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
+
+    # 1. Router Check
+    print("   [Check 1] Router Decision")
+    next_node = route_signal(state)
+    print(f"   -> Result: {next_node}")
+
+    if next_node == "summarizer":
+        print("   âœ… OK: Correctly directed to Summarizer.")
+    else:
+        print(f"   âŒ Failed: Expected 'summarizer', got '{next_node}'.")
+
+    # 2. Summarizer Execution
+    print("\n   [Check 2] Summarizer Execution")
+    # ã“ã“ã§SummarizerãŒèµ°ã‚Šã€è¦ç´„ç”Ÿæˆã¨å±¥æ­´ã®åœ§ç¸®ãŒè¡Œã‚ã‚Œã¾ã™
+    updates = await summarizer(state)
+
+    new_history = updates.get("history_window", [])
+    new_summary = updates.get("summary", "")
+
+    print(f"   Old History Len: {len(dummy_history)}")
+    print(f"   New History Len: {len(new_history)} (Expected: 2)")
+    print(f"   New Summary    : {new_summary}")
+
+    # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
+    if len(new_history) == 2 and len(new_summary) > 10:
+        print("   âœ… OK: History compressed and summary updated.")
+    else:
+        print("   âŒ Failed: History not compressed correctly.")
+
+    # --------------------------------------------------
+    # Case 2: Silence Handler Test (é«˜é€Ÿãƒ‘ã‚¹)
+    # --------------------------------------------------
+    print("\n--------------------------------------------------")
+    print("ğŸ§ª [Test Case 2] SilenceHandler (Fast Path)")
+    print("   Condition: input_type == 'silence'")
+
+    # å…¥åŠ›ã‚’ã€Œæ²ˆé»™ã€ã«è¨­å®š
+    state["input_type"] = "silence"
+
+    # 1. Router Check
+    print("   [Check 1] Router Decision")
+    next_node = route_signal(state)
+    print(f"   -> Result: {next_node}")
+
+    if next_node == "silence_handler":
+        print("   âœ… OK: Correctly directed to SilenceHandler.")
+    else:
+        print(f"   âŒ Failed: Expected 'silence_handler', got '{next_node}'.")
+
+    # 2. SilenceHandler Execution
+    print("\n   [Check 2] SilenceHandler Execution")
+
+    # ç›´å‰ã®å±¥æ­´ã‚„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦è©±é¡Œã‚’æä¾›ã™ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
+    updates = await silence_handler(state)
+    final_sugs = updates.get("final_suggestions", [])
+
+    if final_sugs:
+        sug = final_sugs[0]
+        print(f"   Generated Text: {sug['text']}")
+        print(f"   Speaker       : {sug['speaker']} (Should be Alice or Bob)")
+        print(f"   Type          : {sug['type']}")
+
+        # ç°¡æ˜“è©•ä¾¡: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
+        text = sug["text"]
+        keywords = [
+            "æ˜ ç”»",
+            "Movie",
+            "æ—…è¡Œ",
+            "Travel",
+            "æ¸©æ³‰",
+            "äº¬éƒ½",
+            "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
+            "Netflix",
+        ]
+        if any(w in text for w in keywords):
+            print("   âœ… OK: Topic generated based on common interests/context.")
+        else:
+            print("   âš ï¸ Check Content manually (might be generic).")
+
+        # AIã£ã½ã•ã®ãƒã‚§ãƒƒã‚¯
+        if "ãŠäºŒäººã¯" in text or "è©±é¡Œã‚’å¤‰ãˆã¾ã—ã‚‡ã†" in text:
+            print("   âŒ Failed: Sounding too robotic/AI-like.")
+        else:
+            print("   âœ… OK: Natural phrasing.")
+
+    else:
+        print("   âŒ Failed: No suggestions generated.")
+
+    print("\n==================================================")
+    print("   Tests Completed")
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
